# MusicGen Emotion Interpretability - Complete System Overview

**Last Updated**: October 10, 2024
**Status**: Phase 1 Ready 

---

## The Complete Pipeline

```
┌┐
PHASE 0: COMPLETE 
┘

Text Prompts T5 Encoder Embeddings
┌┐ ┌┐ ┌┐
"happy > T5 > 768-dim 
music" tokenize + encode base embedding 
┘ ┘ ┘


┌┐
DISCOVERY: Emotions ARE encoded here (96% accuracy) 
• Happy vs Sad: 74.5% similarity (25% differentiation) 
• Within-emotion: 56% similarity 
• Between-emotion: 49% similarity 
┘

┌┐
PHASE 1: READY TO START 
┘

T5 Embeddings SAE Training Learned Features
┌┐ ┌┐ ┌┐
768-dim > Sparse > 50-100 
(100 train with Auto- find monosemantic 
samples) L1 sparsity encoder features features 
┘ ┘ ┘

768 6144 768 Feature 42: "joyful"
(8x overcomplete) Feature 108: "sad"
Feature 221: "calm"

┌┐
PHASE 2: PLANNED 
┘

Learned Features Activation Steering MusicGen Output
┌┐ ┌┐ ┌┐
Feature 42: > Add steering > Happier 
"joyful" inject vector to T5 gen music 
(activation) conditioning generated 
┘ ┘ ┘
```

---

## File Organization

### Documentation (Start Here!)

```
MusicGen/

PHASE0_TO_PHASE1_SUMMARY.md PHASE 0 RESULTS & DISCOVERIES
PHASE1_QUICKSTART.md START PHASE 1 (10 min guide)
PHASE1_ROADMAP.md Complete 3-week plan
SYSTEM_OVERVIEW.md ️ This file (system map)
README.md Project overview
START_HERE.md Codebase navigation
```

### Models & Algorithms

```
src/
models/
sparse_autoencoder.py SAE implementation (7686144768)
- L1 sparsity penalty
- Dead feature reinitialization
- Feature tracking

utils/
activation_utils.py MusicGen activation extraction
audio_utils.py Audio processing (librosa)
dataset_utils.py T5 embedding loading & batching
visualization_utils.py Plotting utilities
```

### Experiments & Scripts

```
experiments/

extract_t5_embeddings_at_scale.py Phase 0: Create T5 dataset
Output: 100 embeddings

train_sae_on_t5_embeddings.py Phase 1: Train SAE
Input: T5 embeddings
Output: Trained SAE model

analyze_sae_features.py Phase 1: Analyze features
Input: Trained SAE
Output: Feature interpretations
```

### Data & Results

```
results/

t5_embeddings/ Phase 0 output (COMPLETE )
embeddings.npy 100 × 768 T5 embeddings
labels.npy 100 emotion labels
metadata.json Statistics & analysis
emotion_clustering_pca.png Visualization

sae_training/ Phase 1 output (TO BE CREATED)
[experiment_name]/
best_model.pt Trained SAE weights
config.json Hyperparameters
train_metrics.json Training curves data
training_curves.png Loss/sparsity plots

sae_analysis/ Phase 1 analysis (TO BE CREATED)
feature_emotion_heatmap.png Which features which emotions
analysis_results.json Selectivity scores, etc.
```

---

## Key Components Explained

### 1. T5 Text Embeddings (Phase 0 Discovery)

**What**: 768-dimensional vectors from T5-base encoder

**Why important**: This is WHERE emotions are encoded (not in transformer layers)

**Evidence**:
- 96% classification accuracy (can predict emotion from embedding)
- 25% differentiation between emotions (statistically significant)
- Embeddings cluster by emotion in PCA space

**How to extract**:
```python
from transformers import T5Tokenizer, T5EncoderModel

tokenizer = T5Tokenizer.from_pretrained('t5-base')
encoder = T5EncoderModel.from_pretrained('t5-base')

# Encode prompt
tokens = tokenizer("happy upbeat music", return_tensors='pt')
output = encoder(**tokens)
embedding = output.last_hidden_state.mean(dim=1) # [1, 768]
```

**Data location**: `results/t5_embeddings/embeddings.npy`

---

### 2. Sparse Autoencoder (Phase 1 Tool)

**What**: Neural network that finds sparse, interpretable features

**Architecture**:
```
Input: 768 (T5 embedding)

Encoder: 768 6144 (ReLU activation)

Hidden: 6144 features (only ~50-200 active per sample)

Decoder: 6144 768 (reconstruction)

Output: 768 (reconstructed T5 embedding)
```

**Training objective**:
```
Loss = MSE(input, output) + λ * L1(hidden_activations)
reconstruction sparsity penalty
```

**Why overcomplete (6144 > 768)**:
- Neural networks compress features (superposition)
- Need MORE dimensions to disentangle
- 8x expansion recommended by Anthropic SAE papers

**Implementation**: `src/models/sparse_autoencoder.py`

---

### 3. Feature Selectivity (Phase 1 Metric)

**What**: Measure of how specific a feature is to one emotion

**Formula**:
```
Selectivity = max_emotion(activation_rate) / mean_all_emotions(activation_rate)
```

**Example**:
```
Feature 42 activation rates:
Happy: 80% max
Sad: 10%
Calm: 15%
Energetic: 12%

Mean: (80 + 10 + 15 + 12) / 4 = 29.25%

Selectivity = 80% / 29.25% = 2.74x
```

**Interpretation**:
- Selectivity < 2.0: Not selective (polysemantic)
- Selectivity 2.0-4.0: Moderately selective
- Selectivity > 4.0: Highly selective (monosemantic!)

**Target**: Find 50+ features with selectivity > 2.0

---

## ️ How to Use This System

### Phase 1 Workflow

```
Step 1: Train SAE

$ python3 experiments/train_sae_on_t5_embeddings.py

Expected output:
• Training progress bar (500 steps, ~10 min)
• Model saved to results/sae_training/
• Training curves plot

Success criteria:
Reconstruction MSE < 0.02
L0 = 50-500 active features
< 100 dead features


Step 2: Analyze Features

$ python3 experiments/analyze_sae_features.py

Expected output:
• Feature-emotion heatmap
• List of selective features
• Selectivity scores

Success criteria:
50+ features with selectivity > 2.0
Features cluster by emotion
Top features have clear interpretations


Step 3: Hyperparameter Tuning

Edit CONFIG in train_sae_on_t5_embeddings.py:

l1_coefficient: 1e-3 Try 3e-4, 3e-3, 1e-2

Pick model with:
Best reconstruction (MSE < 0.01)
Good sparsity (L0 = 50-200)
Most interpretable features
```

---

## Math & Theory (Simplified)

### Why Emotions Are in T5 Embeddings

**Hypothesis**: MusicGen pipeline is:
1. **T5 encodes**: Text Emotion representation
2. **Transformer executes**: Emotion representation Audio plan
3. **EnCodec decodes**: Audio plan Waveform

**Evidence from Phase 0**:
- T5 embeddings: 25% emotion differentiation 
- Transformer activations: 5% differentiation 
- Conclusion: Emotion is in INPUT, not PROCESSING

**Analogy**:
- T5 = Recipe ingredients (differ by emotion)
- Transformer = Cooking method (similar regardless)
- Audio = Final dish (tastes different)

We were studying the cooking, not the ingredients!

---

### Why SAEs Find Interpretable Features

**Problem**: Neural networks use superposition
- 768 dimensions encode 1000+ concepts
- Features interfere with each other
- Result: Polysemantic neurons (one neuron = many meanings)

**Solution**: Sparse Autoencoders
- Force sparsity with L1 penalty
- Use overcomplete representation (6144 > 768)
- Result: Monosemantic features (one feature = one meaning)

**Mathematical intuition**:
```
Standard neuron (polysemantic):
neuron_42 = 0.3*"happy" + 0.5*"energetic" + 0.2*"major_key"

SAE feature (monosemantic):
feature_42 = 0.95*"joyful_celebration" + 0.05*noise
```

**Why it works**:
- L1 penalty: Encourages exactly-zero, not small values
- ReLU: Enforces non-negativity (interpretable directions)
- Overcomplete: Enough capacity to separate concepts

---

## Expected Phase 1 Results

### Quantitative Predictions

| Metric | Expected Range | Interpretation |
|--------|---------------|----------------|
| **Reconstruction MSE** | 0.005 - 0.015 | Good reconstruction |
| **L0 (active features)** | 50 - 200 | Sparse enough to interpret |
| **Selective features** | 50 - 150 | Enough for emotion coverage |
| **Top selectivity** | 3.0 - 6.0x | Highly emotion-specific |
| **Dead features** | < 10% | Most features useful |

### Qualitative Predictions

**Expected feature types**:

1. **Emotion-specific**:
- Feature 42: "joyful celebration"
- Feature 108: "melancholic longing"
- Feature 221: "peaceful tranquility"
- Feature 334: "intense aggression"

2. **Emotion-modifying**:
- Feature 15: "intensity" (applies to any emotion)
- Feature 67: "subtlety" (opposite of intensity)

3. **Musical attributes**:
- Feature 89: "acoustic/organic"
- Feature 123: "electronic/synthetic"

4. **Compositional**:
- Combining happy feature + acoustic feature = "acoustic happy music"

---

## Common Issues & Solutions

### Issue: Poor Reconstruction (MSE > 0.05)

**Diagnosis**:
```python
# Check reconstruction quality
model = SparseAutoencoder.load('results/sae_training/best_model.pt')
output = model(test_embeddings, return_loss=True)
print(f"MSE: {output['loss_reconstruction'].item():.4f}")
```

**Solutions**:
1. Decrease L1 coefficient (3e-3 1e-3)
2. Increase expansion factor (8x 12x)
3. Train longer (500 1000 epochs)

---

### Issue: Not Sparse (L0 > 500)

**Diagnosis**:
```python
output = model(test_embeddings, return_aux=True)
print(f"Active features: {output['l0'].item():.0f}")
```

**Solutions**:
1. Increase L1 coefficient (1e-3 3e-3)
2. Check ReLU is working (should see clear 0/non-zero split)

---

### Issue: No Selective Features

**Diagnosis**:
```python
# Run analysis
$ python3 experiments/analyze_sae_features.py

# Check selectivity scores in output
```

**Solutions**:
1. Increase sparsity (higher L1) forces specialization
2. Scale up dataset (100 500 samples) more diverse
3. Increase capacity (8x 12x or 16x) more features

---

## Success Checklist

### Phase 0 (COMPLETE)

- [x] Validated emotion encoding (96% accuracy)
- [x] Identified T5 embeddings as key representation
- [x] Created 100-sample T5 embedding dataset
- [x] Built SAE infrastructure
- [x] Created training & analysis pipelines

### Phase 1 (NEXT)

**Week 1**:
- [ ] Train baseline SAE
- [ ] Hyperparameter sweep
- [ ] Initial feature analysis
- [ ] Document findings

**Week 2**:
- [ ] Generate 400-500 diverse prompts
- [ ] Extract T5 embeddings
- [ ] Retrain on larger dataset
- [ ] Compare to Week 1

**Week 3**:
- [ ] Manual feature inspection
- [ ] Causal validation (ablation tests)
- [ ] Stability tests (multiple seeds)
- [ ] Write Phase 1 report

**Go/No-Go Decision**:
- [ ] 50+ selective features found
- [ ] Features are interpretable
- [ ] Causal tests pass
- [ ] Ready for Phase 2 (activation steering)

---

## Quick Reference

### Commands

```bash
# Activate environment
cd "/Users/lending/Documents/AI PRJ/MusicGen"
source venv/bin/activate

# Train SAE (Phase 1, Step 1)
python3 experiments/train_sae_on_t5_embeddings.py

# Analyze features (Phase 1, Step 2)
python3 experiments/analyze_sae_features.py

# Test SAE implementation
python3 src/models/sparse_autoencoder.py

# Test dataset utilities
python3 src/utils/dataset_utils.py
```

### File Paths

```python
# Input data
T5_EMBEDDINGS = 'results/t5_embeddings/embeddings.npy'
T5_LABELS = 'results/t5_embeddings/labels.npy'
T5_METADATA = 'results/t5_embeddings/metadata.json'

# Model checkpoint (after training)
BEST_MODEL = 'results/sae_training/[experiment]/best_model.pt'

# Analysis results
HEATMAP = 'results/sae_analysis/feature_emotion_heatmap.png'
ANALYSIS = 'results/sae_analysis/analysis_results.json'
```

### Key Hyperparameters

```python
CONFIG = {
'expansion_factor': 8, # 768 * 8 = 6144 hidden dims
'l1_coefficient': 1e-3, # Start here, tune later
'learning_rate': 1e-3, # Adam learning rate
'batch_size': 16, # For 100 samples
'num_epochs': 500, # With early stopping
}
```

---

## Learning Resources

### Papers (Essential)

1. **Bricken et al. (2023)** - "Towards Monosemanticity"
- Core SAE methodology
- Read before Phase 1

2. **Park et al. (2024)** - "Linear Representation Hypothesis"
- Why features are linear directions
- Theory for Phase 2

3. **Copet et al. (2023)** - "MusicGen Paper"
- Architecture details
- Background for the project

### Code Examples

1. **SAE Training**:
- `experiments/train_sae_on_t5_embeddings.py`
- Full implementation with comments

2. **Feature Analysis**:
- `experiments/analyze_sae_features.py`
- Selectivity computation

3. **T5 Embedding Extraction**:
- `experiments/extract_t5_embeddings_at_scale.py`
- Shows full pipeline

---

## Next Action

**To start Phase 1 right now**:

```bash
cd "/Users/lending/Documents/AI PRJ/MusicGen"
source venv/bin/activate
python3 experiments/train_sae_on_t5_embeddings.py
```

Expected time: **10 minutes**
Expected output: **Trained SAE with emotion-selective features**

Then review [PHASE1_QUICKSTART.md](PHASE1_QUICKSTART.md) for next steps!

---

*System ready. All components tested. Phase 1 begins now! *
