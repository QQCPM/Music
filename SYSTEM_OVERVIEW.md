# MusicGen Emotion Interpretability - Complete System Overview

**Last Updated**: October 10, 2024
**Status**: Phase 1 Ready âœ…

---

## ğŸ¯ The Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PHASE 0: COMPLETE âœ…                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Text Prompts                    T5 Encoder              Embeddings
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ "happy   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚   T5    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚  768-dim  â”‚
  â”‚  music"  â”‚   tokenize + encodeâ”‚  base   â”‚            â”‚ embedding â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  DISCOVERY: Emotions ARE encoded here (96% accuracy)            â”‚
  â”‚  â€¢ Happy vs Sad: 74.5% similarity (25% differentiation)         â”‚
  â”‚  â€¢ Within-emotion: 56% similarity                                â”‚
  â”‚  â€¢ Between-emotion: 49% similarity                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 1: READY TO START ğŸš€                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  T5 Embeddings                  SAE Training          Learned Features
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  768-dim  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚  Sparse  â”‚ â”€â”€â”€â”€â”€â”€â”€> â”‚ 50-100       â”‚
  â”‚ (100      â”‚   train with    â”‚  Auto-   â”‚  find    â”‚ monosemantic â”‚
  â”‚  samples) â”‚   L1 sparsity   â”‚  encoder â”‚  featuresâ”‚ features     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚                         â”‚
                          768 â†’ 6144 â†’ 768          Feature 42: "joyful"
                          (8x overcomplete)         Feature 108: "sad"
                                                    Feature 221: "calm"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PHASE 2: PLANNED                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Learned Features            Activation Steering        MusicGen Output
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Feature 42:  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Add steering    â”‚ â”€â”€â”€â”€>  â”‚ Happier     â”‚
  â”‚ "joyful"     â”‚  inject   â”‚ vector to T5    â”‚ gen    â”‚ music       â”‚
  â”‚ (activation) â”‚           â”‚ conditioning    â”‚        â”‚ generated   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ File Organization

### ğŸ“š Documentation (Start Here!)

```
MusicGen/
â”‚
â”œâ”€â”€ PHASE0_TO_PHASE1_SUMMARY.md    â† ğŸ“Š PHASE 0 RESULTS & DISCOVERIES
â”œâ”€â”€ PHASE1_QUICKSTART.md           â† ğŸš€ START PHASE 1 (10 min guide)
â”œâ”€â”€ PHASE1_ROADMAP.md              â† ğŸ“‹ Complete 3-week plan
â”œâ”€â”€ SYSTEM_OVERVIEW.md             â† ğŸ—ºï¸  This file (system map)
â”œâ”€â”€ README.md                      â† ğŸ“– Project overview
â””â”€â”€ START_HERE.md                  â† ğŸ“‚ Codebase navigation
```

### ğŸ§  Models & Algorithms

```
src/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ sparse_autoencoder.py     â† SAE implementation (768â†’6144â†’768)
â”‚                                     - L1 sparsity penalty
â”‚                                     - Dead feature reinitialization
â”‚                                     - Feature tracking
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ activation_utils.py        â† MusicGen activation extraction
    â”œâ”€â”€ audio_utils.py             â† Audio processing (librosa)
    â”œâ”€â”€ dataset_utils.py           â† T5 embedding loading & batching
    â””â”€â”€ visualization_utils.py     â† Plotting utilities
```

### ğŸ§ª Experiments & Scripts

```
experiments/
â”‚
â”œâ”€â”€ extract_t5_embeddings_at_scale.py  â† Phase 0: Create T5 dataset
â”‚                                          Output: 100 embeddings
â”‚
â”œâ”€â”€ train_sae_on_t5_embeddings.py      â† Phase 1: Train SAE
â”‚                                          Input: T5 embeddings
â”‚                                          Output: Trained SAE model
â”‚
â””â”€â”€ analyze_sae_features.py            â† Phase 1: Analyze features
                                           Input: Trained SAE
                                           Output: Feature interpretations
```

### ğŸ“Š Data & Results

```
results/
â”‚
â”œâ”€â”€ t5_embeddings/                 â† Phase 0 output (COMPLETE âœ…)
â”‚   â”œâ”€â”€ embeddings.npy                 100 Ã— 768 T5 embeddings
â”‚   â”œâ”€â”€ labels.npy                     100 emotion labels
â”‚   â”œâ”€â”€ metadata.json                  Statistics & analysis
â”‚   â””â”€â”€ emotion_clustering_pca.png     Visualization
â”‚
â”œâ”€â”€ sae_training/                  â† Phase 1 output (TO BE CREATED)
â”‚   â””â”€â”€ [experiment_name]/
â”‚       â”œâ”€â”€ best_model.pt              Trained SAE weights
â”‚       â”œâ”€â”€ config.json                Hyperparameters
â”‚       â”œâ”€â”€ train_metrics.json         Training curves data
â”‚       â””â”€â”€ training_curves.png        Loss/sparsity plots
â”‚
â””â”€â”€ sae_analysis/                  â† Phase 1 analysis (TO BE CREATED)
    â”œâ”€â”€ feature_emotion_heatmap.png    Which features â†’ which emotions
    â””â”€â”€ analysis_results.json          Selectivity scores, etc.
```

---

## ğŸ”¬ Key Components Explained

### 1. T5 Text Embeddings (Phase 0 Discovery)

**What**: 768-dimensional vectors from T5-base encoder

**Why important**: This is WHERE emotions are encoded (not in transformer layers)

**Evidence**:
- 96% classification accuracy (can predict emotion from embedding)
- 25% differentiation between emotions (statistically significant)
- Embeddings cluster by emotion in PCA space

**How to extract**:
```python
from transformers import T5Tokenizer, T5EncoderModel

tokenizer = T5Tokenizer.from_pretrained('t5-base')
encoder = T5EncoderModel.from_pretrained('t5-base')

# Encode prompt
tokens = tokenizer("happy upbeat music", return_tensors='pt')
output = encoder(**tokens)
embedding = output.last_hidden_state.mean(dim=1)  # [1, 768]
```

**Data location**: `results/t5_embeddings/embeddings.npy`

---

### 2. Sparse Autoencoder (Phase 1 Tool)

**What**: Neural network that finds sparse, interpretable features

**Architecture**:
```
Input:  768 (T5 embedding)
   â†“
Encoder: 768 â†’ 6144 (ReLU activation)
   â†“
Hidden: 6144 features (only ~50-200 active per sample)
   â†“
Decoder: 6144 â†’ 768 (reconstruction)
   â†“
Output: 768 (reconstructed T5 embedding)
```

**Training objective**:
```
Loss = MSE(input, output) + Î» * L1(hidden_activations)
       â””â”€ reconstruction   â””â”€ sparsity penalty
```

**Why overcomplete (6144 > 768)**:
- Neural networks compress features (superposition)
- Need MORE dimensions to disentangle
- 8x expansion recommended by Anthropic SAE papers

**Implementation**: `src/models/sparse_autoencoder.py`

---

### 3. Feature Selectivity (Phase 1 Metric)

**What**: Measure of how specific a feature is to one emotion

**Formula**:
```
Selectivity = max_emotion(activation_rate) / mean_all_emotions(activation_rate)
```

**Example**:
```
Feature 42 activation rates:
  Happy:      80%  â† max
  Sad:        10%
  Calm:       15%
  Energetic:  12%

  Mean: (80 + 10 + 15 + 12) / 4 = 29.25%

  Selectivity = 80% / 29.25% = 2.74x
```

**Interpretation**:
- Selectivity < 2.0: Not selective (polysemantic)
- Selectivity 2.0-4.0: Moderately selective
- Selectivity > 4.0: Highly selective (monosemantic!)

**Target**: Find 50+ features with selectivity > 2.0

---

## âš™ï¸ How to Use This System

### Phase 1 Workflow

```
Step 1: Train SAE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
$ python3 experiments/train_sae_on_t5_embeddings.py

Expected output:
  â€¢ Training progress bar (500 steps, ~10 min)
  â€¢ Model saved to results/sae_training/
  â€¢ Training curves plot

Success criteria:
  âœ… Reconstruction MSE < 0.02
  âœ… L0 = 50-500 active features
  âœ… < 100 dead features


Step 2: Analyze Features
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
$ python3 experiments/analyze_sae_features.py

Expected output:
  â€¢ Feature-emotion heatmap
  â€¢ List of selective features
  â€¢ Selectivity scores

Success criteria:
  âœ… 50+ features with selectivity > 2.0
  âœ… Features cluster by emotion
  âœ… Top features have clear interpretations


Step 3: Hyperparameter Tuning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Edit CONFIG in train_sae_on_t5_embeddings.py:

l1_coefficient: 1e-3 â†’ Try 3e-4, 3e-3, 1e-2

Pick model with:
  âœ… Best reconstruction (MSE < 0.01)
  âœ… Good sparsity (L0 = 50-200)
  âœ… Most interpretable features
```

---

## ğŸ§® Math & Theory (Simplified)

### Why Emotions Are in T5 Embeddings

**Hypothesis**: MusicGen pipeline is:
1. **T5 encodes**: Text â†’ Emotion representation
2. **Transformer executes**: Emotion representation â†’ Audio plan
3. **EnCodec decodes**: Audio plan â†’ Waveform

**Evidence from Phase 0**:
- T5 embeddings: 25% emotion differentiation âœ…
- Transformer activations: 5% differentiation âŒ
- Conclusion: Emotion is in INPUT, not PROCESSING

**Analogy**:
- T5 = Recipe ingredients (differ by emotion)
- Transformer = Cooking method (similar regardless)
- Audio = Final dish (tastes different)

We were studying the cooking, not the ingredients!

---

### Why SAEs Find Interpretable Features

**Problem**: Neural networks use superposition
- 768 dimensions encode 1000+ concepts
- Features interfere with each other
- Result: Polysemantic neurons (one neuron = many meanings)

**Solution**: Sparse Autoencoders
- Force sparsity with L1 penalty
- Use overcomplete representation (6144 > 768)
- Result: Monosemantic features (one feature = one meaning)

**Mathematical intuition**:
```
Standard neuron (polysemantic):
  neuron_42 = 0.3*"happy" + 0.5*"energetic" + 0.2*"major_key"

SAE feature (monosemantic):
  feature_42 = 0.95*"joyful_celebration" + 0.05*noise
```

**Why it works**:
- L1 penalty: Encourages exactly-zero, not small values
- ReLU: Enforces non-negativity (interpretable directions)
- Overcomplete: Enough capacity to separate concepts

---

## ğŸ“Š Expected Phase 1 Results

### Quantitative Predictions

| Metric | Expected Range | Interpretation |
|--------|---------------|----------------|
| **Reconstruction MSE** | 0.005 - 0.015 | Good reconstruction |
| **L0 (active features)** | 50 - 200 | Sparse enough to interpret |
| **Selective features** | 50 - 150 | Enough for emotion coverage |
| **Top selectivity** | 3.0 - 6.0x | Highly emotion-specific |
| **Dead features** | < 10% | Most features useful |

### Qualitative Predictions

**Expected feature types**:

1. **Emotion-specific**:
   - Feature 42: "joyful celebration"
   - Feature 108: "melancholic longing"
   - Feature 221: "peaceful tranquility"
   - Feature 334: "intense aggression"

2. **Emotion-modifying**:
   - Feature 15: "intensity" (applies to any emotion)
   - Feature 67: "subtlety" (opposite of intensity)

3. **Musical attributes**:
   - Feature 89: "acoustic/organic"
   - Feature 123: "electronic/synthetic"

4. **Compositional**:
   - Combining happy feature + acoustic feature = "acoustic happy music"

---

## ğŸš¨ Common Issues & Solutions

### Issue: Poor Reconstruction (MSE > 0.05)

**Diagnosis**:
```python
# Check reconstruction quality
model = SparseAutoencoder.load('results/sae_training/best_model.pt')
output = model(test_embeddings, return_loss=True)
print(f"MSE: {output['loss_reconstruction'].item():.4f}")
```

**Solutions**:
1. Decrease L1 coefficient (3e-3 â†’ 1e-3)
2. Increase expansion factor (8x â†’ 12x)
3. Train longer (500 â†’ 1000 epochs)

---

### Issue: Not Sparse (L0 > 500)

**Diagnosis**:
```python
output = model(test_embeddings, return_aux=True)
print(f"Active features: {output['l0'].item():.0f}")
```

**Solutions**:
1. Increase L1 coefficient (1e-3 â†’ 3e-3)
2. Check ReLU is working (should see clear 0/non-zero split)

---

### Issue: No Selective Features

**Diagnosis**:
```python
# Run analysis
$ python3 experiments/analyze_sae_features.py

# Check selectivity scores in output
```

**Solutions**:
1. Increase sparsity (higher L1) â†’ forces specialization
2. Scale up dataset (100 â†’ 500 samples) â†’ more diverse
3. Increase capacity (8x â†’ 12x or 16x) â†’ more features

---

## ğŸ¯ Success Checklist

### âœ… Phase 0 (COMPLETE)

- [x] Validated emotion encoding (96% accuracy)
- [x] Identified T5 embeddings as key representation
- [x] Created 100-sample T5 embedding dataset
- [x] Built SAE infrastructure
- [x] Created training & analysis pipelines

### ğŸ”² Phase 1 (NEXT)

**Week 1**:
- [ ] Train baseline SAE
- [ ] Hyperparameter sweep
- [ ] Initial feature analysis
- [ ] Document findings

**Week 2**:
- [ ] Generate 400-500 diverse prompts
- [ ] Extract T5 embeddings
- [ ] Retrain on larger dataset
- [ ] Compare to Week 1

**Week 3**:
- [ ] Manual feature inspection
- [ ] Causal validation (ablation tests)
- [ ] Stability tests (multiple seeds)
- [ ] Write Phase 1 report

**Go/No-Go Decision**:
- [ ] 50+ selective features found
- [ ] Features are interpretable
- [ ] Causal tests pass
- [ ] Ready for Phase 2 (activation steering)

---

## ğŸ“ Quick Reference

### Commands

```bash
# Activate environment
cd "/Users/lending/Documents/AI PRJ/MusicGen"
source venv/bin/activate

# Train SAE (Phase 1, Step 1)
python3 experiments/train_sae_on_t5_embeddings.py

# Analyze features (Phase 1, Step 2)
python3 experiments/analyze_sae_features.py

# Test SAE implementation
python3 src/models/sparse_autoencoder.py

# Test dataset utilities
python3 src/utils/dataset_utils.py
```

### File Paths

```python
# Input data
T5_EMBEDDINGS = 'results/t5_embeddings/embeddings.npy'
T5_LABELS = 'results/t5_embeddings/labels.npy'
T5_METADATA = 'results/t5_embeddings/metadata.json'

# Model checkpoint (after training)
BEST_MODEL = 'results/sae_training/[experiment]/best_model.pt'

# Analysis results
HEATMAP = 'results/sae_analysis/feature_emotion_heatmap.png'
ANALYSIS = 'results/sae_analysis/analysis_results.json'
```

### Key Hyperparameters

```python
CONFIG = {
    'expansion_factor': 8,         # 768 * 8 = 6144 hidden dims
    'l1_coefficient': 1e-3,        # Start here, tune later
    'learning_rate': 1e-3,         # Adam learning rate
    'batch_size': 16,              # For 100 samples
    'num_epochs': 500,             # With early stopping
}
```

---

## ğŸ“ Learning Resources

### Papers (Essential)

1. **Bricken et al. (2023)** - "Towards Monosemanticity"
   - Core SAE methodology
   - Read before Phase 1

2. **Park et al. (2024)** - "Linear Representation Hypothesis"
   - Why features are linear directions
   - Theory for Phase 2

3. **Copet et al. (2023)** - "MusicGen Paper"
   - Architecture details
   - Background for the project

### Code Examples

1. **SAE Training**:
   - `experiments/train_sae_on_t5_embeddings.py`
   - Full implementation with comments

2. **Feature Analysis**:
   - `experiments/analyze_sae_features.py`
   - Selectivity computation

3. **T5 Embedding Extraction**:
   - `experiments/extract_t5_embeddings_at_scale.py`
   - Shows full pipeline

---

## ğŸš€ Next Action

**To start Phase 1 right now**:

```bash
cd "/Users/lending/Documents/AI PRJ/MusicGen"
source venv/bin/activate
python3 experiments/train_sae_on_t5_embeddings.py
```

Expected time: **10 minutes**
Expected output: **Trained SAE with emotion-selective features**

Then review [PHASE1_QUICKSTART.md](PHASE1_QUICKSTART.md) for next steps!

---

*System ready. All components tested. Phase 1 begins now! ğŸµğŸ”¬*
