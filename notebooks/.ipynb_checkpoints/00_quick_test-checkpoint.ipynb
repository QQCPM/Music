{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Test: MusicGen Setup & First Experiments\n",
    "\n",
    "This notebook tests your installation and runs your first experiments.\n",
    "\n",
    "**Expected time**: 15-20 minutes\n",
    "\n",
    "**What you'll do**:\n",
    "1. Load MusicGen Large (3.3B)\n",
    "2. Generate music samples\n",
    "3. Extract activations\n",
    "4. Visualize differences between emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "import torch\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "from src.utils.activation_utils import ActivationExtractor, ActivationDataset\n",
    "from src.utils.visualization_utils import plot_activation_statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    print(\"Using CPU (this will be slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load MusicGen Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MusicGen Large (3.3B parameters)...\")\n",
    "print(\"This may take 2-5 minutes on first load...\")\n",
    "\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-large')\n",
    "model.set_generation_params(duration=8)  # 8 second samples\n",
    "\n",
    "print(\"✅ Model loaded!\")\n",
    "print(f\"   Model size: 3.3B parameters\")\n",
    "print(f\"   Sample rate: {model.sample_rate}\")\n",
    "print(f\"   Number of layers: {len(model.lm.layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Music Samples\n",
    "\n",
    "Let's generate music with different emotions and listen to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts for different emotions\n",
    "prompts = {\n",
    "    'happy': \"upbeat cheerful pop music with bright melody and positive energy\",\n",
    "    'sad': \"melancholic piano ballad with sorrowful emotional melody\",\n",
    "    'calm': \"peaceful ambient meditation music with gentle soft tones\",\n",
    "    'energetic': \"high energy electronic dance music with intense driving beat\"\n",
    "}\n",
    "\n",
    "print(\"Generating music samples...\")\n",
    "print(\"(This will take ~30-60 seconds per sample)\")\n",
    "print()\n",
    "\n",
    "generated_samples = {}\n",
    "\n",
    "for emotion, prompt in prompts.items():\n",
    "    print(f\"Generating '{emotion}' music...\")\n",
    "    wav = model.generate([prompt])\n",
    "    generated_samples[emotion] = wav[0]\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = f'../results/sample_{emotion}'\n",
    "    audio_write(output_path, wav[0].cpu(), model.sample_rate, strategy=\"loudness\")\n",
    "    print(f\"   Saved to: {output_path}.wav\")\n",
    "\n",
    "print()\n",
    "print(\"✅ All samples generated!\")\n",
    "print(\"   Listen to them in the results/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Activations\n",
    "\n",
    "Now let's extract activations from multiple layers to see how the model represents different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation extractor\n",
    "# We'll sample 5 layers from the 24 total layers\n",
    "layers_to_extract = [0, 6, 12, 18, 24]\n",
    "\n",
    "print(f\"Extracting activations from layers: {layers_to_extract}\")\n",
    "extractor = ActivationExtractor(model, layers=layers_to_extract)\n",
    "\n",
    "# Create dataset to store activations\n",
    "dataset = ActivationDataset()\n",
    "\n",
    "print()\n",
    "print(\"Generating and extracting activations...\")\n",
    "\n",
    "for emotion, prompt in prompts.items():\n",
    "    print(f\"  {emotion}...\")\n",
    "    \n",
    "    # Generate with activation capture\n",
    "    wav = extractor.generate([prompt])\n",
    "    \n",
    "    # Get and store activations\n",
    "    activations = extractor.get_activations()\n",
    "    dataset.add(\n",
    "        activations=activations,\n",
    "        prompt=prompt,\n",
    "        label=emotion\n",
    "    )\n",
    "    \n",
    "    extractor.clear_activations()  # Free memory\n",
    "\n",
    "print()\n",
    "print(f\"✅ Extracted activations from {len(dataset)} samples\")\n",
    "\n",
    "# Save dataset for later analysis\n",
    "dataset.save('../results/emotion_activations.pt')\n",
    "print(\"   Saved to: results/emotion_activations.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Activation Patterns\n",
    "\n",
    "Let's visualize how activations differ across emotions and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare activation statistics for happy vs sad\n",
    "happy_activations, _ = dataset[0]  # First sample (happy)\n",
    "sad_activations, _ = dataset[1]    # Second sample (sad)\n",
    "\n",
    "print(\"Activation shapes:\")\n",
    "for layer_name, act in happy_activations.items():\n",
    "    print(f\"  {layer_name}: {act.shape}\")\n",
    "    # Shape: [batch=1, sequence_length, d_model=2048]\n",
    "\n",
    "print()\n",
    "print(\"Comparing 'happy' vs 'sad' music activations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation statistics for happy music\n",
    "fig = plot_activation_statistics(\n",
    "    happy_activations,\n",
    "    save_path='../results/happy_activation_stats.png'\n",
    ")\n",
    "plt.suptitle('Activation Statistics: Happy Music', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Plot activation statistics for sad music\n",
    "fig = plot_activation_statistics(\n",
    "    sad_activations,\n",
    "    save_path='../results/sad_activation_stats.png'\n",
    ")\n",
    "plt.suptitle('Activation Statistics: Sad Music', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare Emotions\n",
    "\n",
    "Let's compute some simple metrics to see if emotions are represented differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.activation_utils import cosine_similarity, analyze_activation_statistics\n",
    "\n",
    "# Compare layer 12 (middle layer) activations\n",
    "happy_layer12 = happy_activations['layer_12']\n",
    "sad_layer12 = sad_activations['layer_12']\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = cosine_similarity(happy_layer12, sad_layer12)\n",
    "\n",
    "print(f\"Cosine similarity between happy and sad (layer 12): {similarity:.4f}\")\n",
    "print()\n",
    "\n",
    "if similarity < 0.9:\n",
    "    print(\"✅ The activations are quite different!\")\n",
    "    print(\"   This suggests the model represents emotions differently.\")\n",
    "else:\n",
    "    print(\"⚠️  The activations are very similar.\")\n",
    "    print(\"   This might mean emotions aren't clearly separated in this layer.\")\n",
    "\n",
    "print()\n",
    "print(\"Statistics for layer 12:\")\n",
    "print(\"\\nHappy music:\")\n",
    "happy_stats = analyze_activation_statistics(happy_layer12)\n",
    "for key, val in happy_stats.items():\n",
    "    print(f\"  {key}: {val:.4f}\")\n",
    "\n",
    "print(\"\\nSad music:\")\n",
    "sad_stats = analyze_activation_statistics(sad_layer12)\n",
    "for key, val in sad_stats.items():\n",
    "    print(f\"  {key}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What you just did**:\n",
    "\n",
    "1. ✅ Loaded MusicGen Large (3.3B parameters)\n",
    "2. ✅ Generated music for 4 emotions (happy, sad, calm, energetic)\n",
    "3. ✅ Extracted internal activations from 5 transformer layers\n",
    "4. ✅ Visualized activation patterns\n",
    "5. ✅ Compared representations between emotions\n",
    "\n",
    "**Key observations**:\n",
    "- MusicGen has 24 transformer layers, each with d_model=2048 dimensions\n",
    "- Activations differ across layers (early vs late)\n",
    "- Different emotions may produce different activation patterns\n",
    "\n",
    "**Next steps**:\n",
    "\n",
    "1. **Listen to the generated music** in `results/sample_*.wav`\n",
    "2. **Experiment more**:\n",
    "   - Try different prompts\n",
    "   - Extract from more layers\n",
    "   - Generate more samples per emotion\n",
    "3. **Start Phase 0 learning** ([docs/phase0_roadmap.md](../docs/phase0_roadmap.md))\n",
    "4. **Read about sparse autoencoders** to understand how to interpret these activations\n",
    "\n",
    "This is just the beginning! The real research starts when you:\n",
    "- Train SAEs to find interpretable features\n",
    "- Use UMAP to visualize emotion clustering\n",
    "- Test causal interventions\n",
    "- Apply activation steering\n",
    "\n",
    "**See you in Phase 1!** 🎵🔬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
